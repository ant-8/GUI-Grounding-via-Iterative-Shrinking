{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa22d00c-8517-4687-a2b9-cf2c64779b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 04:33:45.145245: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-14 04:33:45.170833: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-14 04:33:45.644772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a8bd92ac524763b930d82f940b8fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c228b2a2aeb418cb4e7043dd1544bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pipelines import get_gemini_pipes, get_gpt4o_pipes\n",
    "from utils import *\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from numpy import np\n",
    "from region_traverser import *\n",
    "\n",
    "model_name = \"OS-Copilot/OS-Atlas-Base-7B\"\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "ds = load_screenspot_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6de9bccf-590f-4de7-93c3-fe67b033cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinate_prediction(images, target):\n",
    "    content = [{\"type\": \"image\", \"image\": image} for image in images] + [\n",
    "        {\"type\": \"text\", \"text\": f\"In the attached UI screenshot, what is the bbox of the element corresponding to the command \\\"{target}\\\"? Write your final answer in the following format (x1, y1, x2, y2)\"},\n",
    "    ]\n",
    "    if len(images) > 1:\n",
    "        content[-1][\"text\"] = \"You are given a screenshot represented as two images. The first is fully zoomed out only for context. The second image is zoomed to the area of interest and should be the focus. Write coordinates relative to the first image. \" + content[-1][\"text\"]\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content,\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Preparation for inference\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    #print(text)\n",
    "    prefix = \"\" if len(images) == 1 else \"Second image: \"\n",
    "    text += f\"<|object_ref_start|>{prefix}{target}<|object_ref_end|><|box_start|>(\"\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    #print(output_text)\n",
    "    coords_str = output_text[0].split(\"<|box_end|>\")[0]\n",
    "    #print(coords_str)\n",
    "    coords = coords_str.replace('(', '').replace(')', '').replace('[', '').replace(']', '').split(',')\n",
    "    x1, y1, x2, y2 = map(int, coords)\n",
    "    bbox = [x1, y1, x2, y2]\n",
    "    \n",
    "    midpoint_x = (x1 + x2) / 2\n",
    "    midpoint_y = (y1 + y2) / 2\n",
    "    \n",
    "    result = (midpoint_x, midpoint_y), bbox\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1590263-b0ab-4105-91f3-04d010b43765",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = ds['web']['icon'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b88edaeb-8f00-48f6-a2c6-8528ae0d2c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds['desktop']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "52d2f077-3bd5-451d-8f35-77d50b952102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [04:35<00:00,  2.75s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "for row in tqdm(test_set):\n",
    "    res = eval_row(row)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "77a27724-4a0a-4b14-ab88-832739b95733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 106/106 [04:53<00:00,  2.77s/it]\n"
     ]
    }
   ],
   "source": [
    "for row in tqdm(ds['web']['icon'][100:]):\n",
    "    res = eval_row(row)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "49c556df-b584-42b1-a741-6c255e218e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7572815533980582"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in results if x]) / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "abc09dfd-16a2-4efb-b348-c4a44663fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_indices = [i for i in range(len(results)) if results[i] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8029a1c8-674f-44be-921b-e59c1a0704f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 23,\n",
       " 24,\n",
       " 29,\n",
       " 30,\n",
       " 36,\n",
       " 48,\n",
       " 49,\n",
       " 71,\n",
       " 73,\n",
       " 76,\n",
       " 77,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 89,\n",
       " 90,\n",
       " 96,\n",
       " 97]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0491eff-6e97-4e60-bb3e-23abe3e71bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [02:17<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "baseline_results = []\n",
    "for row in tqdm(test_set):\n",
    "    res = eval_row_baseline(row)\n",
    "    baseline_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e9dfcc88-bdd1-46e6-a088-2873365fc3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6990291262135923"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in baseline_results if x]) / len(baseline_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "39b15e4a-53bc-485f-a457-4a5de7375570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36, 85, 86]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_indices_baseline = [i for i in range(len(baseline_results)) if baseline_results[i] == False]\n",
    "[x for x in wrong_indices if x not in wrong_indices_baseline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d3cbb73b-3a00-4403-810f-b8ad320beca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'web - text': 230,\n",
       " 'web - icon': 206,\n",
       " 'mobile - text': 273,\n",
       " 'mobile - icon': 229,\n",
       " 'desktop - text': 194,\n",
       " 'desktop - icon': 140}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb6a81ad-ea66-4b35-9d70-97824de1a63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 230/230 [15:55<00:00,  4.16s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 206/206 [14:18<00:00,  4.17s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 273/273 [18:52<00:00,  4.15s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 229/229 [15:49<00:00,  4.14s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 194/194 [13:22<00:00,  4.14s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 140/140 [09:34<00:00,  4.11s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "devices = [\"web\", \"mobile\", \"desktop\"]\n",
    "ui_types = [\"text\", \"icon\"]\n",
    "\n",
    "weights = {}\n",
    "\n",
    "for d in devices:\n",
    "    for t in ui_types:\n",
    "        test_set = ds[d][t]\n",
    "        weights[f\"{d} - {t}\"] = len(test_set)\n",
    "\n",
    "eval_result = {}\n",
    "\n",
    "for d in devices:\n",
    "    for t in ui_types:\n",
    "        test_set = ds[d][t]\n",
    "        results = []\n",
    "        for row in tqdm(test_set):\n",
    "            res = eval_row(row)\n",
    "            results.append(res)\n",
    "        eval_result[f\"{d} - {t}\"] = len([x for x in results if x]) / len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba06f307-050e-4f1e-887e-d45bd07b8b4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43meval_result\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_result' is not defined"
     ]
    }
   ],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb76e7e3-65b5-4d1d-b9c7-46e3b4ad88ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8411949685534591"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.average(list(eval_result.values()), weights=list(weights.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6436c543-e999-436f-b854-eac5a8c40ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8268014648235166"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(eval_result.values()))/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7980ee6-695d-497e-ab7c-081bc4722ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def render_crosshair(image, x, y):\n",
    "    \"\"\"\n",
    "    Draws a crosshair intersecting at the given (x, y) coordinates on the image.\n",
    "\n",
    "    Parameters:\n",
    "        image (PIL.Image): The input image to draw the crosshair on.\n",
    "        x (int): The x-coordinate of the crosshair intersection.\n",
    "        y (int): The y-coordinate of the crosshair intersection.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: A new image with the crosshair rendered.\n",
    "    \"\"\"\n",
    "    # Create a copy of the image to draw on\n",
    "    rendered_image = image.copy()\n",
    "    draw = ImageDraw.Draw(rendered_image)\n",
    "\n",
    "    # Get image dimensions\n",
    "    width, height = image.size\n",
    "\n",
    "    # Draw horizontal and vertical lines for the crosshair\n",
    "    line_color = \"red\"  # Color of the crosshair\n",
    "    line_width = 2      # Width of the crosshair lines\n",
    "\n",
    "    # Draw vertical line\n",
    "    draw.line([(x, 0), (x, height)], fill=line_color, width=line_width)\n",
    "\n",
    "    # Draw horizontal line\n",
    "    draw.line([(0, y), (width, y)], fill=line_color, width=line_width)\n",
    "\n",
    "    return rendered_image\n",
    "\n",
    "#r = await image_pipe([image], \"Determine the x, y coordinates of the New Folder button. Think step-by-step carefully in a series of numbered steps. Return your answer as JSON with two keys: thoughts (string), x (int) and y (int)\", schema=1)\n",
    "#r = json.loads(r)\n",
    "#print(r)\n",
    "#render_crosshair(image, r[\"x\"], r[\"y\"]).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
