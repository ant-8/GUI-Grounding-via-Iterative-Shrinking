{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa22d00c-8517-4687-a2b9-cf2c64779b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 02:14:48.012788: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-12 02:14:48.039257: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-12 02:14:48.518290: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f178b0f768427ea17f06004d7ed4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pipelines import get_gemini_pipes, get_gpt4o_pipes\n",
    "from utils import *\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from region_traverser import *\n",
    "\n",
    "model_name = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "ds = load_screenspot_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6de9bccf-590f-4de7-93c3-fe67b033cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinate_prediction(images, target):\n",
    "    content = [{\"type\": \"image\", \"image\": image} for image in images] + [\n",
    "        {\"type\": \"text\", \"text\": f\"In the attached UI screenshot, calculate the exact position of the element corresponding to the command \\\"{target}\\\". Write your answer in the form of (x, y) where each x and y is normalized between 0 and 1. Examples: (0.25, 0.25) is the top-left, (0.75, 0.75) is the bottom-right.\"},\n",
    "    ]\n",
    "    if len(images) > 1:\n",
    "        content[-1][\"text\"] = \"You are given a screenshot represented as two images. The second image is fully zoomed out only for context. Write coordinates relative to the first image. \" + content[-1][\"text\"]\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content,\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Preparation for inference\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    #print(text)\n",
    "    prefix = \"\" if len(images) == 1 else \"Second image: \"\n",
    "    #text += f\"<|object_ref_start|>{prefix}{target}<|object_ref_end|><|box_start|>\"\n",
    "    text += f\"The exact coordinate of the element corresponding to the command \\\"{target}\\\" is (x,y) = (\"\n",
    "    #text += f\"To get the exact position of the element corresponding to the command \\\"{target}\\\", let's first think step by step:\"\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=2048)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    #print(output_text[0])\n",
    "    #coords_str = output_text[0].split(\"(\")[1].split(\")\")[0]\n",
    "    #print(coords_str)\n",
    "    #coords = coords_str.replace('\\'', '').replace('(', '').replace('(', '').replace(')', '').replace('[', '').replace(']', '').split(',')\n",
    "    x, y = extract_tuple_from_string(output_text[0])#map(int, coords)\n",
    "    \n",
    "    result = (x*999, y*999)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a1590263-b0ab-4105-91f3-04d010b43765",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = ds['web']['icon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b88edaeb-8f00-48f6-a2c6-8528ae0d2c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds['desktop']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e80789c1-ce27-496e-a4a8-6fc71165c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_quadrant(midpoint, prediction_coord):\n",
    "    mid_x, mid_y = midpoint\n",
    "    pred_x, pred_y = prediction_coord\n",
    "\n",
    "    if pred_x < mid_x and pred_y < mid_y:\n",
    "        return 0\n",
    "    elif pred_x >= mid_x and pred_y < mid_y:\n",
    "        return 1\n",
    "    elif pred_x < mid_x and pred_y >= mid_y:\n",
    "        return 2\n",
    "    elif pred_x >= mid_x and pred_y >= mid_y:\n",
    "        return 3\n",
    "\n",
    "dim = 999\n",
    "\n",
    "def eval_row_baseline(row, verbose=False):\n",
    "    target = row['target']\n",
    "    og_size = row['image'].size\n",
    "    current_region = row['image'].resize((dim,dim))\n",
    "    x, y = get_coordinate_prediction([current_region], target)\n",
    "    x = x * current_region.size[0]\n",
    "    x = y * current_region.size[1]\n",
    "    porp_x = x / dim\n",
    "    porp_y = y / dim\n",
    "\n",
    "    x = og_size[0] * porp_x\n",
    "    y = og_size[1] * porp_y\n",
    "\n",
    "    if verbose:\n",
    "        print(target)\n",
    "        print(current_region.size)\n",
    "        print(x,y)\n",
    "        render_crosshair(row['image'], x, y).convert(\"RGB\").show()\n",
    "    return is_in_bbox(row['bbox'], x, y)\n",
    "\n",
    "def eval_row(row, verbose=False):\n",
    "    target = row['target']\n",
    "    current_region = row['image'].copy()\n",
    "\n",
    "    traverser = RegionTraverser(current_region)\n",
    "    if verbose: print(target)\n",
    "    k = 3\n",
    "    #render_crosshair_center(current_region).show()\n",
    "    for i in range(k):\n",
    "        images_prompt = [render_crosshair_center(current_region.resize((dim, dim)))]\n",
    "        #if i > 0:\n",
    "            #images_prompt = images_prompt + [result_image.resize((dim, dim))]  \n",
    "        prediction_coord = get_coordinate_prediction(images_prompt, target)\n",
    "        pred_x, pred_y = prediction_coord\n",
    "        #if i == 0: images_prompt[0].show()\n",
    "        #if verbose and i == 0: draw_bbox_on_image(current_region.resize((dim, dim)), pred_bbox).convert(\"RGB\").show()\n",
    "        #print(list(pred_bbox))\n",
    "        #render_crosshair(current_region, pred_x, pred_y).convert(\"RGB\").show()\n",
    "        \n",
    "        if verbose: print(prediction_coord)\n",
    "        if i != k-1:\n",
    "            traverser.consume_coordinate(pred_x, pred_y)#, (pred_bbox[2] - pred_bbox[0])*16, (pred_bbox[3] - pred_bbox[1])*16)\n",
    "            result_image = traverser.get_highlighted_image()\n",
    "            current_region = traverser.get_cropped_image().resize((dim,dim))\n",
    "\n",
    "        if verbose: result_image.convert(\"RGB\").show()\n",
    "    final_bbox = traverser.get_bounding_box()\n",
    "\n",
    "    last_porp_x = pred_x / dim\n",
    "    last_porp_y = pred_y / dim\n",
    "    \n",
    "    delta_x = (final_bbox[2] - final_bbox[0]) * last_porp_x\n",
    "    delta_y = (final_bbox[3] - final_bbox[1]) * last_porp_y\n",
    "\n",
    "    x,y = final_bbox[0] + delta_x, final_bbox[1] + delta_y\n",
    "    if verbose:\n",
    "        render_crosshair(current_region, pred_x, pred_y).convert(\"RGB\").show()\n",
    "        render_crosshair(row['image'], x, y).convert(\"RGB\").show()\n",
    "        #draw_bbox_on_image(row['image'], pred_bbox).convert(\"RGB\").show()\n",
    "    return is_in_bbox(row['bbox'], x, y)\n",
    "\n",
    "#eval_row(test_set[14], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "52d2f077-3bd5-451d-8f35-77d50b952102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 206/206 [13:09<00:00,  3.83s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "for row in tqdm(test_set):\n",
    "    res = eval_row(row)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "49c556df-b584-42b1-a741-6c255e218e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5242718446601942"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in results if x]) / len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "abc09dfd-16a2-4efb-b348-c4a44663fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_indices = [i for i in range(len(results)) if results[i] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8029a1c8-674f-44be-921b-e59c1a0704f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True, True, False, True, True, True, False, False, True]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a0491eff-6e97-4e60-bb3e-23abe3e71bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 206/206 [04:25<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "baseline_results = []\n",
    "for row in tqdm(test_set):\n",
    "    res = eval_row_baseline(row)\n",
    "    baseline_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e9dfcc88-bdd1-46e6-a088-2873365fc3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11650485436893204"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in baseline_results if x]) / len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "39b15e4a-53bc-485f-a457-4a5de7375570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 99]"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_indices_baseline = [i for i in range(len(baseline_results)) if baseline_results[i] == False]\n",
    "[x for x in wrong_indices if x not in wrong_indices_baseline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d3cbb73b-3a00-4403-810f-b8ad320beca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'web - text': 230,\n",
       " 'web - icon': 206,\n",
       " 'mobile - text': 273,\n",
       " 'mobile - icon': 229,\n",
       " 'desktop - text': 194,\n",
       " 'desktop - icon': 140}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cb6a81ad-ea66-4b35-9d70-97824de1a63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 230/230 [14:31<00:00,  3.79s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 206/206 [13:01<00:00,  3.80s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 273/273 [17:07<00:00,  3.76s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 229/229 [14:22<00:00,  3.77s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 194/194 [12:09<00:00,  3.76s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 140/140 [08:48<00:00,  3.77s/it]\n"
     ]
    }
   ],
   "source": [
    "devices = [\"web\", \"mobile\", \"desktop\"]\n",
    "ui_types = [\"text\", \"icon\"]\n",
    "\n",
    "weights = {}\n",
    "\n",
    "for d in devices:\n",
    "    for t in ui_types:\n",
    "        test_set = ds[d][t]\n",
    "        weights[f\"{d} - {t}\"] = len(test_set)\n",
    "\n",
    "eval_result = {}\n",
    "\n",
    "for d in devices:\n",
    "    for t in ui_types:\n",
    "        test_set = ds[d][t]\n",
    "        results = []\n",
    "        for row in tqdm(test_set):\n",
    "            res = eval_row(row)\n",
    "            results.append(res)\n",
    "        eval_result[f\"{d} - {t}\"] = len([x for x in results if x]) / len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ba06f307-050e-4f1e-887e-d45bd07b8b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'web - text': 0.7043478260869566,\n",
       " 'web - icon': 0.49514563106796117,\n",
       " 'mobile - text': 0.673992673992674,\n",
       " 'mobile - icon': 0.5502183406113537,\n",
       " 'desktop - text': 0.7938144329896907,\n",
       " 'desktop - icon': 0.5357142857142857}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cb76e7e3-65b5-4d1d-b9c7-46e3b4ad88ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6312893081761006"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.average(list(eval_result.values()), weights=list(weights.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
